{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dad4a00-5b9c-45de-a984-b7ad6c915ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0930e99-aa07-443b-832a-4ff1e0cbb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eea917f-191f-42c3-9105-f452de6b6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e32fed-2df1-40e9-9f55-cc07c1935e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2402fd4-ffa4-4442-b697-508de9046cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d463b6b-7654-4c2b-af03-b1ceb8238895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "あなたはAI言語モデルアシスタントです。\n",
    "あなたのタスクは、与えられたユーザーの質問に対して5つの異なるバージョンを生成し、ベクターデータベースから関連文書を検索することです。\n",
    "ユーザーの質問に対して複数の視点を生成することで、距離ベースの類似性検索の限界をユーザーが克服できるように支援することが目標です。\n",
    "これらの代替質問を改行で区切って入力してください。\n",
    "元の質問: {question}\n",
    "\"\"\"\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspective\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a881de-86e8-4e58-9225-06d47a6e854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15703/1816738893.py:6: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattend_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattend_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"LLMエージェントにとってのタスク分解とはなにか？\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831b58de-b17f-4b75-a342-707f01b53c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'タスク分解とは、複雑なタスクを小さな管理しやすいサブゴールに分割することです。エージェントは大きなタスクを複数の小さなステップに分解し、それを実行するための計画を立てることが重要です。これにより、複雑なタスクを効率的に処理することが可能となります。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"\n",
    "このコンテキストに基づいて以下の質問に答えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dae1e4-c4b3-46c9-8e65-6d2da3300347",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "あなたは、1つの入力クエリに基づいて複数の検索クエリを生成する便利なアシスタントです。\\n\n",
    "{question}に関連する複数の検索クエリを生成します。\\n\n",
    "出力（4クエリ）\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c6c0d1-cabf-42d0-9080-7f3f8cf75c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f613523e-60cf-451b-ac8e-50486cb32935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da748785-686c-4f8e-a41f-449961f2fd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMエージェントにとってのタスク分解とは、複雑なタスクをより小さな、管理しやすいサブゴールに分解することを指します。これにより、エージェントは複雑なタスクを効率的に処理することができます。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"\n",
    "このコンテキストに基づいて以下の質問に答えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba241dbf-650d-4c9c-8a29-c69a47cd5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "あなたは、入力された質問に関連する複数のサブ質問を生成する、役に立つアシスタントです。\\n\n",
    "目標は、入力された質問を、単独で回答できるサブ問題/サブ質問のセットに分解することです。\\n\n",
    "{question} に関連する複数の検索クエリを生成します。\\n\n",
    "出力 (3 クエリ):\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95cbef25-b76f-4e12-9392-1f42652d8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "generate_queries_decomposition = (prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "question = \"LLMのエージェントシステムのメインコンポーネントはなんですか？\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f89bc9c5-656a-4d71-af23-3ad244a7eb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. LLMのエージェントシステムのメインコンポーネントは何ですか？',\n",
       " '2. LLMのエージェントシステムにおけるメインコンポーネントの機能は何ですか？',\n",
       " '3. LLMのエージェントシステムにおいて、メインコンポーネントはどのようにして機能しますか？']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7033f50b-ca6c-4e30-b961-cac73c0d30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "回答が必要な質問は次のとおりです。\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "利用可能な背景情報に関する質問と回答のペアは次のとおりです。\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "質問に関連する追加のコンテキストは次のとおりです。\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "上記のコンテキストと、背景情報に関する質問と回答のペアを使用して、質問に回答してください。\\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93e84064-0000-4e5d-972b-87daeb9791c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-5.2\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acdd779a-dcb8-4a6d-82de-c711ffe863b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMを中核コントローラ（脳）とするエージェントシステムでは、LLM単体の推論・生成を **Planning（計画）** と **Memory（記憶）**（＋必要に応じて **Tool use（ツール利用）**）が補完し、次のように連携して機能します。\\n\\n- **Planning（計画）**  \\n  - **サブゴール分解**：与えられた大きなタスクを、小さく実行可能な手順・目標に分割し、順序立てて進められる形にします。  \\n  - **振り返り・改善（Reflection/Refinement）**：実行結果や過去の行動を自己批評・自己反省し、誤りや不足を学習して次の行動計画や最終出力を改善します（試行錯誤を前提に反復的に品質を上げる）。\\n\\n- **Memory（記憶）**  \\n  - **短期記憶**：プロンプト内（インコンテキスト）にある直近の会話・状況を保持して、その場の推論に使います。  \\n  - **長期記憶**：外部ストレージ（例：ベクターストア）に情報を蓄積し、必要時に検索・想起して、複数回のやり取りや長いタスクでも一貫性と継続性を保ちます。\\n\\n- **Tool use（ツール利用：補完的だが重要）**  \\n  - LLMが外部APIや検索、コード実行などを呼び出して、モデル内部にない最新情報の取得や実世界への操作を行います。必要に応じて「APIが必要か→どのAPIか→入力を調整して再試行→結果を踏まえて回答」という反復も行います。\\n\\n要するに、**Planningが“何をどう進めるか”を組み立て、Memoryが“過去や必要知識”を供給し、Tool useが“外部で取得・実行”を担い**、LLMがそれらを統合して自律的にタスクを遂行します。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5800c9d2-ddb9-4886-a110-5aecf01a8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "    rag_results = []\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke(\n",
    "            {\"context\": retrieved_docs, \"question\": sub_question}\n",
    "        )\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d10fac-ef97-4912-af3a-f927cb75e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMのエージェントシステムのメインコンポーネントは、**LLMを中核コントローラ（脳）**として、それを補完する **Planning（計画）・Memory（記憶）・Tool use（ツール利用）** です。  \\n特にPlanningには、**タスクのサブゴール分解**や**自己反省／改善（reflection/refinement）**が含まれます。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"\n",
    "Q+A のペアを以下に示します。\n",
    "\n",
    "{context}\n",
    "\n",
    "これらを使用して、質問 {question} に対する回答を作成してください。\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a7ae-bb84-42cd-af74-7f59f9f63df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
